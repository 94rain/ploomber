{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essential: Core concepts\n",
    "\n",
    "\n",
    "## Ploomber's core: Tasks, Products, DAG and Clients\n",
    "\n",
    "To get started with ploomber you only have to learn four concepts:\n",
    "\n",
    "1. Task. A unit of work that takes some input and produces a persistent change\n",
    "2. Product. A persistent change *produced* by a Task (e.g. a file in the local filesystem, a table in a remote database)\n",
    "3. DAG. A collection of Tasks, used to specify dependencies among them (use output from Task A as input for Task B)\n",
    "4. Client. An object that communicates with an external system (e.g. a database)\n",
    "\n",
    "There is a standard [Task API](../api.rst#ploomber.tasks.Task) defined by an abstract class, this is also true for [Products](../api.rst#ploomber.products.Product) and [Clients](../api.rst#ploomber.clients.Client). Which means you only have to learn the concept once and all concrete classes will have the same API.\n",
    "\n",
    "\n",
    "## The DAG lifecycle: Declare, render, build\n",
    "\n",
    "A DAG goes through three steps before being executed:\n",
    "\n",
    "1. Declaration. A DAG is created and Tasks are added to it\n",
    "2. Rendering. Placeholders are resolved and validation is performed on Task inputs\n",
    "3. Building. All *outdated* Tasks are executed in the appropriate order (run upstream task dependencies first)\n",
    "\n",
    "### Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from ploomber import DAG\n",
    "from ploomber.tasks import PythonCallable, SQLUpload, SQLScript\n",
    "from ploomber.clients import SQLAlchemyClient\n",
    "from ploomber.products import File, SQLiteRelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest Task is `PythonCallable`, which takes a callable (e.g. a function) as its first argument. The only requirement for the functions is to have a `product`\n",
    "argument, if the task has dependencies, it must have an upstream argument as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _one_task(product):\n",
    "    pd.DataFrame({'one_column': [1, 2, 3]}).to_csv(str(product))\n",
    "\n",
    "def _another_task(upstream, product):\n",
    "    df = pd.read_csv(str(upstream['one']))\n",
    "    df['another_column'] = df['one_column'] + 1\n",
    "    df.to_csv(str(product))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonCallable: another -> File(another_file.csv)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantate our DAG\n",
    "dag = DAG()\n",
    "\n",
    "# instantiate two tasks and add them to the DAG\n",
    "one_task = PythonCallable(_one_task, File('one_file.csv'), dag, name='one')\n",
    "another_task = PythonCallable(_another_task, File('another_file.csv'), dag, name='another')\n",
    "\n",
    "# declare dependencies: another_task depends on one_task\n",
    "one_task >> another_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the previous function definitions we use `str(product)`, since products are custom objects, they will not work directly when used as parameters to the `DataFrame.to_csv()` function, since our products are `File` objects, using `str` will return a the path as a string. Other products implement different logic, for example a `SQLRelation` returns a `schema.name` string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* File(/path/to/some/file) to str: \"/path/to/some/file\"\n",
      "* SQLiteRelation(my_table) to str: \"my_table\"\n"
     ]
    }
   ],
   "source": [
    "f = File('/path/to/some/file')\n",
    "print('* {} to str: \"{}\"'.format(repr(f), str(f)))\n",
    "\n",
    "# SQLiteRelation takes a ('schema', 'name', 'kind') or a ('name', 'kind') tuple where kind is 'table' or 'view'\n",
    "t = SQLiteRelation(('my_table', 'table'))\n",
    "print('* {} to str: \"{}\"'.format(repr(t), str(t)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rendering\n",
    "\n",
    "To generate a Product, Tasks use a combination of inputs and a `source`. The kind of source depends on the kind of Task, `PythonCallable` uses a Python function as source, `SQLScript` uses a string with SQL code as source, `SQLUpload` uses a string to a file as source. Rendering is the process where any necessary preparation and validation to the source take place.\n",
    "\n",
    "One use case for this is to avoid redudant code. If a Task is declared to have an upstream dependency, it means that it will take the upstream Product as input, instead of declaring the Product twice, we can refer to it in the downstream task using a placeholder. Let's see an example using `SQLUpload`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SQLUpload: my_table -> SQLiteRelation(my_table)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clients are objects that communicate with external systems, such as databases\n",
    "client = SQLAlchemyClient('sqlite:///my_db.db')\n",
    "\n",
    "# Tasks that use clients have a client argument, but you can also define DAG-level clients\n",
    "dag.clients[SQLUpload] = client\n",
    "dag.clients[SQLiteRelation] = client\n",
    "dag.clients[SQLScript] = client\n",
    "\n",
    "# Source is defined as a placeholder: take the product from the upstream task\n",
    "# named \"another\" and use it as source\n",
    "my_table = SQLUpload(source='{{upstream[\"another\"]}}',\n",
    "                     product=SQLiteRelation(('my_table', 'table')),\n",
    "                     dag=dag,\n",
    "                     name='my_table')\n",
    "\n",
    "another_task >> my_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74fc10fb926466cab3099cf61c79d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "my_table.source as string: \"another_file.csv\"\n"
     ]
    }
   ],
   "source": [
    "# resolve placeholders by rendering\n",
    "dag.render()\n",
    "\n",
    "# let's see the rendered value:\n",
    "print('my_table.source as string: \"{}\"'.format(str(my_table.source)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important use case for placeholders are parametrized SQL queries. `SQLScript` runs SQL code in a database that creates a table or a view. Since ploomber requires sources (SQL code) and products (a table/view) to be declared separately we can use placeholders to only declare the product once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SQLScript: second_table -> SQLiteRelation(second_table)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = \"\"\"\n",
    "-- product is a placeholder\n",
    "DROP TABLE IF EXISTS {{product}};\n",
    "\n",
    "CREATE TABLE {{product}}\n",
    "AS SELECT * FROM {{upstream[\"my_table\"]}}\n",
    "WHERE one_column = 1\n",
    "\"\"\"\n",
    "\n",
    "# instead of declaring \"second_table\" twice, we declare it in product and refer to it in source\n",
    "second_table = SQLScript(source=source,\n",
    "                         product=SQLiteRelation(('second_table', 'table')),\n",
    "                         dag=dag,\n",
    "                         name='second_table')\n",
    "\n",
    "my_table >> second_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "754682e289d94c0db5a370059fa79f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DAG(\"No name\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_table.source:\n",
      "\n",
      "-- product is a placeholder\n",
      "DROP TABLE IF EXISTS second_table;\n",
      "\n",
      "CREATE TABLE second_table\n",
      "AS SELECT * FROM my_table\n",
      "WHERE one_column = 1\n"
     ]
    }
   ],
   "source": [
    "print('second_table.source:\\n{}'.format(str(second_table.source)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ploomber uses [jinja2](https://jinja.palletsprojects.com/en/2.11.x/api/) for rendering, which opens a wide range of possibilities rendering SQL source code. Note that this time, we didn't use the `str` operator explicitely as we did for PythonCallable, this is because jinja automatically casts objects to strings.\n",
    "\n",
    "Before building our dag, let's take a look at the current status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c03fb81c355467e8bb10295688ea72d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name        </th><th>Last updated                        </th><th>Outdated dependencies  </th><th>Outdated code  </th><th>Product         </th><th>Doc (short)  </th><th>Location                        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>one         </td><td>18 seconds ago (Mar 15, 20 at 14:47)</td><td>False                  </td><td>False          </td><td>one_file.csv    </td><td>             </td><td><ipython-input-2-e7312d45e800>:1</td></tr>\n",
       "<tr><td>another     </td><td>18 seconds ago (Mar 15, 20 at 14:47)</td><td>False                  </td><td>False          </td><td>another_file.csv</td><td>             </td><td><ipython-input-2-e7312d45e800>:4</td></tr>\n",
       "<tr><td>my_table    </td><td>18 seconds ago (Mar 15, 20 at 14:47)</td><td>False                  </td><td>False          </td><td>my_table        </td><td>             </td><td>                                </td></tr>\n",
       "<tr><td>second_table</td><td>18 seconds ago (Mar 15, 20 at 14:47)</td><td>False                  </td><td>False          </td><td>second_table    </td><td>             </td><td>                                </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "name          Last updated                          Outdated dependencies    Outdated code    Product           Doc (short)    Location\n",
       "------------  ------------------------------------  -----------------------  ---------------  ----------------  -------------  --------------------------------\n",
       "one           18 seconds ago (Mar 15, 20 at 14:47)  False                    False            one_file.csv                     <ipython-input-2-e7312d45e800>:1\n",
       "another       18 seconds ago (Mar 15, 20 at 14:47)  False                    False            another_file.csv                 <ipython-input-2-e7312d45e800>:4\n",
       "my_table      18 seconds ago (Mar 15, 20 at 14:47)  False                    False            my_table\n",
       "second_table  18 seconds ago (Mar 15, 20 at 14:47)  False                    False            second_table"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build\n",
    "\n",
    "Once rendering is done, we can build our DAG. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607a91e0665d4ebbbe3f2a084552fe1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d31545b55e147599848a829697890ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name        </th><th>Ran?  </th><th style=\"text-align: right;\">  Elapsed (s)</th><th style=\"text-align: right;\">  Percentage</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>one         </td><td>False </td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">           0</td></tr>\n",
       "<tr><td>another     </td><td>False </td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">           0</td></tr>\n",
       "<tr><td>my_table    </td><td>False </td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">           0</td></tr>\n",
       "<tr><td>second_table</td><td>False </td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">           0</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "name          Ran?      Elapsed (s)    Percentage\n",
       "------------  ------  -------------  ------------\n",
       "one           False               0             0\n",
       "another       False               0             0\n",
       "my_table      False               0             0\n",
       "second_table  False               0             0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time we run our pipeline, all Tasks are executed, but the real power of ploomber is running builds over and over again. Ploomber keeps track of each Task's status and only executed outdated ones, since we just built our pipeline, nothing will run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a6568fe91f4f538fa2764fa2464389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c394c2fa8fcd411d9b99a99b28de4528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>name        </th><th>Ran?  </th><th style=\"text-align: right;\">  Elapsed (s)</th><th style=\"text-align: right;\">  Percentage</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>one         </td><td>False </td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">           0</td></tr>\n",
       "<tr><td>another     </td><td>False </td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">           0</td></tr>\n",
       "<tr><td>my_table    </td><td>False </td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">           0</td></tr>\n",
       "<tr><td>second_table</td><td>False </td><td style=\"text-align: right;\">            0</td><td style=\"text-align: right;\">           0</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "name          Ran?      Elapsed (s)    Percentage\n",
       "------------  ------  -------------  ------------\n",
       "one           False               0             0\n",
       "another       False               0             0\n",
       "my_table      False               0             0\n",
       "second_table  False               0             0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dag.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task status\n",
    "\n",
    "Upon sucessful execution, a Task will save metadata along with the Product, to keep track of status in subsequent builds. Once a DAG is built (even if some tasks fail), another call to `dag.build()` will only trigger execution on outdated tasks. A task is run if any of the following conditions is true:\n",
    "\n",
    "1. The Products doesn't exist (e.g. when a Task is run for the first time)\n",
    "2. No metadata (e.g. when a Task crashes)\n",
    "3. Any upstream source changed (e.g. an upstream SQL script changed)\n",
    "4. Task's own source changed\n",
    "\n",
    "These rules enable the following use cases:\n",
    "\n",
    "1. Fast incremental builds: Modify any Task source, next build will only run outdated Tasks\n",
    "2. Crash recovery: If a DAG crashes, the next run will start where it was interrupted\n",
    "\n",
    "\n",
    "### Task parameters\n",
    "\n",
    "There is one last remaining Task argument to explain: `params`, they are optional parameters whose effect varies depending on the kind of Task. `PythonCallable` just passes them when calling the underlying function, Tasks that take SQL code as source, pass them directly to the source (they are available as placeholders), `NotebookRunner` (which runs Jupyter notebooks), passes them as parameters using [papermill](https://github.com/nteract/papermill).\n",
    "\n",
    "Let's take a look at the `params` of our previous DAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_task params:\n",
      "\t {'product': File(one_file.csv)}\n",
      "another_task params:\n",
      "\t {'upstream': Upstream({'one': File(one_file.csv)}), 'product': File(another_file.csv)}\n"
     ]
    }
   ],
   "source": [
    "print('one_task params:\\n\\t', one_task.params)\n",
    "print('another_task params:\\n\\t', another_task.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we didn't pass any `param` to the tasks, `product` and `upstream` are automateically added after doing `DAG.render()`, that's why we see those parameters.\n",
    "\n",
    "\n",
    "As a general advice, it is best to keep `params` short, their primary use case is for creating dynamic DAGs (whose number of Tasks is determined using control structures). Dynamic DAGs are covered in a more advanced tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
